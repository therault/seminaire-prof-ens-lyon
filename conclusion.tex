\section{Conclusion and Future Work}

\frame{
  \frametitle{Current Practice for Fault Tolerance in HPC}

  Most applications do not have \textcolor{red}{any} fault-tolerance mechanisms

  \textcolor{blue}{Long running applications} mostly implement
  \begin{itemize}
  \item application-level checkpointing
  \item On disk (PFS)
    \begin{itemize}
    \item Often using libraries (VeloC, HDF5, ADIOS2, ...)
    \item Often with local cache / local staging
    \end{itemize}
  \item Coordinated checkpoint and rollback recovery
  \item With full application-level synchronization
  \item More or less periodic (application-dependent)
  \item But not often using the optimal checkpointing period
    \begin{itemize}
    \item MTBF is often unknown, and \emph{allocations} define the period
    \end{itemize}
  \end{itemize}

  \begin{overlayarea}{\linewidth}{0cm}
    \only<2>{%
      \vspace{-6cm}
      \begin{center}
        \begin{minipage}{.7\linewidth}
          \begin{alertblock}{Faults are not a problem}

            Last 10 years:
            \begin{itemize}
            \item MTBF of \textcolor{blue}{systems} appear to remained capped
            \item \# nodes in Top10 has \emph{decreased}
            \item Computing power of nodes has significantly increased thanks to \emph{many GPUs} / node
            \end{itemize}
          \end{alertblock}
        \end{minipage}
      \end{center}
    }
  \end{overlayarea}%
}

\begin{frame}
  \frametitle{Reasons to continue research on FT for HPC}

  \begin{itemize}
  \item Scale remains the adversary
    \begin{itemize}
    \item End of Dennard scaling (ca 2006)
    \item Moore's Law survives (more or less) by doubling cores (manycore)
    \item or thanks to addition of accelerators
    \end{itemize}
  \item \quesley Will we continue to double the MTBF of nodes at the same speed we double their power?
  \item \quesley Market argument: no-one will commission a supercomputer with $MTBF < \sim 12h$
    \begin{itemize}
    \item \frownie See anecdotal evidence with Sunway TaihuLight
    \item \smiley What is the cost of keeping up with the component reliability increase?\\
      What is the cost of introducing software fault tolerance?\\
      (SW\_Qsim, GB 2021, uses a resilient all-reduce because of failures in new Sunway)
    \item \smiley 'the likelyhood for a supercomputer running for weeks on end is approximately zero' -- Jensen Huang's GTC Keynote, March 18, 2024.
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Silent Data Corruption}

  SDCs are not well evaluated\footnote{In general we need more studies on faults in leadership systems and high-end systems}

  Intuition:
  \begin{itemize}
  \item They happen more often as the amount of memory grows
  \item They happen more often as the computational intensity grows
  \end{itemize}

  If the intuition is right, they will happen more and more frequently

  Application-specific fault tolerance might help
  \begin{itemize}
  \item Not necessarily (or not only) to correct failures
  \item Mostly to detect failures
  \end{itemize}

  Overhead is negligible!

  \begin{center}
    \textcolor{red}{Advocating for hardened/trustworthy version of popular computational libraries}
  \end{center}  
\end{frame}

\begin{frame}
  \frametitle{Other reasons to continue research on FT for HPC}

  \begin{itemize}
  \item Energy consumption becomes a problem (both economical and ecological)
    \begin{itemize}
    \item Current policy is to not bill allocation if a node within the allocation fails
    \item Some platforms are switching to bill-per-Joule
    \item Does it make sense to get 'free' Joules thanks to a node failure?
    \item[] ~
    \item Cloud-HPC convergence: CPUs are billed as soon as allocated, even if the application cannot be deployed
    \end{itemize}
  \item Using stranded energy in Cloud/HPC
    \begin{itemize}
    \item Non-dispatchable renewable energy sources to power HPC
    \item $\Rightarrow$ Power cost varies with time (soft version)
    \item $\Rightarrow$ Power capping varies with time (hard version)
    \item $\Rightarrow$ Job scheduler may have to kill apps. Resilience can help.
    \end{itemize}
  \end{itemize}
\end{frame}

\frame{
  \frametitle{Resilience Techniques Applied to Manage Stragglers}

  \begin{itemize}
  \item Stragglers (slow processes) $\sim$ Failed Processes
    \begin{itemize}
    \item (In fully asynchronous systems, they are indistinguishable)
    \item ABFT techniques: obtain $n-k$ results over $n$ to decide
    \item Extend MPI/ULFM for relaxed collectives / cancellable sends?
      \begin{itemize}
      \item e.g. (All)Reduce with only $k < np$ participants?
      \item e.g. Discard contributions from slow processes?
      \end{itemize}
    \end{itemize}
  \end{itemize}
}

\frame{
  \frametitle{Applying Algorithm Based Fault Tolerance to TBRS}

  \begin{itemize}
  \item ABFT: bulk-synchronous 'by nature'
    \begin{itemize}
    \item Need to keep the cheksum valid at each step
    \item Efficient \emph{because} checksum is updated by only extending synchronous operation
    \end{itemize}
  \item TBRS: asynchronous 'by nature'
    \begin{itemize}
    \item Add strict control flow to keep checksum up to date? \frownie
    \item How to react to failures? Conditional DAG?
    \end{itemize}
  \item ABFT + TBRS:
    \begin{itemize}
    \item Extend DAG with checksum / checksum update tasks
    \item Introduce scheduling priorities to make sure checksums are updated
    \item Define alternative paths to produce data, associate cost with paths
      \begin{itemize}
      \item A block can be computed by inversing the checksum
      \item but preferable path is to update directly
      \end{itemize}
    \end{itemize}
  \end{itemize}
}

\frame{
  \frametitle{Optimal Checkpointing and Task-Based Runtime Systems}
  
    Scheduling checkpoints optimally in TBRS is NP-complete!

    Yet, a TBRS is a parallel application

    And we know what is the \emph{optimal} checkpoint period of a parallel application.
}

\frame{
  \frametitle{Looking back at hierarchical/noncoordinated checkpointing}
  
    'Narrow Gap' of applicability for hierarchical checkpointing

    Advantages of noncoordinated checkpointing:
    \begin{itemize}
    \item no synchronization overheads (\quesley -- but communications are impacted)
    \item checkpoints are staggered (\quesley -- but staging-out of checkpoints works in coordinated)
    \item only failed processes need to restart (\quesley -- but surviving processes need to wait in tightly-coupled applications)
      \begin{itemize}
      \item \smiley noncoordinated should be revisited for \emph{moldable} applications
      \item \smiley TBRS with work migration are ideal candidates
      \end{itemize}
    \end{itemize}
}

\frame{
  \frametitle{Future Work}

  \begin{itemize}
  \item Energy! Silent Errors!
  \item Stragglers
  \item ABFT + TBRS
  \item Optimality of checkpointing in TBRS
  \item Noncoordinated Checkpointing in TBRS
  \end{itemize}

  \vspace{\fill}
  
  \centering Thank you

  \vspace{\fill}
  
  \centering Questions?

  \vspace{\fill}

    \begin{overlayarea}{\linewidth}{0cm}
    \only<2>{%
      \vspace{-7cm}
      \begin{center}
        \begin{minipage}{.9\linewidth}
          \begin{block}{Nothing about AI !?!}

            Roll-forward approaches are probably interesting for AI applications:
            \begin{itemize}
            \item Relaxed collectives / stragglers management
            \item Completely ignore slow processes contribution?
            \end{itemize}

            Using Deep Learning / Reinforced Learning:
            \begin{itemize}
            \item Place checkpoints in TBRS?
            \item Generate missing data for iterative/converging applications
            \item Silent Data Corruption Detector?
            \end{itemize}
          \end{block}
        \end{minipage}
      \end{center}
    }
  \end{overlayarea}%

}

\miniframesoff
\section*{}


